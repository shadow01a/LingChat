# NeoChat 配置文件

# 调试与日志
debug:
  mode: true # 开启/关闭开发者模式。设置为True/False将开关Debug日志。

# 路径配置
paths:
  logs: "logs/run_logs"
  saves: "user_data/saves"
  story_packs: "data/story_packs"
  characters: "data/characters"
  player_characters: "data/player_characters"
  user_config: "user_data/user_config.json"

# RAG (Retrieval Augmented Generation) 设置
rag:
  enabled: false # 是否启用RAG功能 (当前为占位符)
  retrieval_count: 3
  context_m_before: 2
  context_n_after: 2

# --------------------------------------------------------------------------
# LLM (大语言模型) 配置 - [核心重构部分]
# --------------------------------------------------------------------------
llm:
  # 当前激活的 LLM 服务商。这里的名字必须与下面 providers 列表中的一个键匹配。
  active_provider: "deepseek_official"

  # API 请求的全局超时时间 (秒)
  timeout_seconds: 180

  # 对话历史长度限制
  conversation_history_limit: 50
  free_time_history_limit: 15
  ai_choice_history_context_count: 8
  conversation_settings:
    # 是否合并连续的用户（user）消息。
    # 开启后，如果LLM的上下文中出现连续的多条"role: user"消息，
    # 它们会被自动合并为一条，内容用换行符分隔。
    # 这有助于避免某些模型在处理此类输入时可能出现的性能下降或混乱。
    # true: 开启合并 | false: 保持原样
    merge_consecutive_user_messages: true

  # 服务商配置列表
  providers:
    # 1. DeepSeek 官方 API
    deepseek_official:
      type: "openai_compatible" # 客户端类型，指向我们的实现
      api_key: "${DEEPSEEK_API_KEY}" # 从 .env 文件读取 DEEPSEEK_API_KEY
      base_url: "https://api.deepseek.com"
      # 对于标准 OpenAI 格式，路径通常是 /v1/chat/completions
      # DeepSeek 比较特殊，它的路径就是 /chat/completions
      api_path: "/chat/completions"
      default_model: "deepseek-chat"
      default_parameters:
        temperature: 0.7
        max_tokens: 4096

    # 2. 标准 OpenAI 接口
    openai_official:
      type: "openai_compatible"
      api_key: "${OPENAI_API_KEY}"
      base_url: "https://api.openai.com"
      api_path: "/v1/chat/completions" # 这是标准的 OpenAI 路径
      default_model: "gpt-4o"
      default_parameters:
        temperature: 0.7
        max_tokens: 4096

    # 3. Ollama (本地运行)
    ollama_local:
      type: "openai_compatible"
      api_key: "ollama" # Ollama 的 API key 可以是任意非空字符串
      base_url: "http://localhost:11434"
      api_path: "/v1/chat/completions" # Ollama 同样兼容 OpenAI v1 路径
      default_model: "qwen2:7b" # 替换为你自己用 `ollama pull` 下载的模型
      default_parameters:
        temperature: 0.8
        # 注意: 本地模型通常没有严格的 max_tokens 限制，但可以设置以控制输出长度

    # 4. LM Studio (本地运行)
    lm_studio_local:
      type: "openai_compatible"
      api_key: "lm-studio" # LM Studio 的 key 也是任意字符串
      base_url: "http://localhost:1234" # 默认端口
      api_path: "/v1/chat/completions"
      default_model: "local-model" # 在 LM Studio UI 中加载的模型会显示在这里
      default_parameters:
        temperature: 0.7

    # 5. 阿里云百炼 (DashScope)
    aliyun_bailian:
      type: "openai_compatible"
      api_key: "${BAILIAN_API_KEY}"
      base_url: "https://dashscope.aliyuncs.com"
      api_path: "/api/v1/services/aigc/text-generation/generation" # 百炼的路径
      default_model: "qwen-turbo" # 例如 qwen-turbo, qwen-plus 等
      default_parameters:
        temperature: 0.8
        # 百炼的额外参数可以通过 kwargs 传递，例如 'enable_search': true

    # 6. 硅基流动 (SiliconFlow)
    siliconflow:
      type: "openai_compatible"
      api_key: "${SILICONFLOW_API_KEY}"
      base_url: "https://api.siliconflow.cn"
      api_path: "/v1/chat/completions"
      default_model: "deepseek-ai/deepseek-v2-chat" # 替换为你需要的模型
      default_parameters:
        temperature: 0.7
        max_tokens: 8192

    # 7. 火山引擎 (VolcEngine)
    volcengine:
      type: "openai_compatible"
      api_key: "${VOLCENGINE_API_KEY}" # 火山引擎的 key 格式可能不同，但仍放在这里
      base_url: "https://maas-api.ml-platform-cn-beijing.volces.com" # 以北京区域为例
      api_path: "/api/v1/chat/completions"
      default_model: "Skylark2-pro-4k" # 替换为你需要的模型
      default_parameters:
        temperature: 0.7